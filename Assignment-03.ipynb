{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447ef48f",
   "metadata": {},
   "source": [
    "# Assignment 03 - Basic RAG Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46df295",
   "metadata": {},
   "source": [
    "## Load Environment Variables\n",
    "All the environment variables, APIS keys etc are loaded in the `.env` file. Load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36318d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  Loaded environment variables from .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"â„¹ï¸  Loaded environment variables from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5613f",
   "metadata": {},
   "source": [
    "## Define Data Sources\n",
    "\n",
    "Real Meta PDFs from their Investor Relations website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644ce3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data sources loaded!\n",
      "\n",
      "Available quarters: Q1, Q2, Q3, Q4\n",
      "Document types per quarter: press_release, presentation, earnings_call, followup_qa\n",
      "\n",
      "Total documents to process: 16 PDFs\n"
     ]
    }
   ],
   "source": [
    "# Real Meta earnings data sources\n",
    "data_sources = {\n",
    "    \"Q1\": {\n",
    "        \"press_release\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q1/Meta-03-31-2025-Exhibit-99-1-Final.pdf\",\n",
    "        \"presentation\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q1/Earnings-Presentation-Q1-2025-FINAL.pdf\",\n",
    "        \"earnings_call\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q1/Transcripts/META-Q1-2025-Earnings-Call-Transcript-1.pdf\",\n",
    "        \"followup_qa\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q1/Transcripts/META-Q1-2025-Follow-Up-Call-Transcript.pdf\"\n",
    "    },\n",
    "    \"Q2\": {\n",
    "        \"press_release\": \"https://s21.q4cdn.com/399680738/files/doc_downloads/Meta-06-30-2025-Exhibit-99-1-FINAL.pdf\",\n",
    "        \"presentation\": \"https://s21.q4cdn.com/399680738/files/doc_downloads/Earnings-Presentation-Q2-2025-FINAL.pdf\",\n",
    "        \"earnings_call\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q2/META-Q2-2025-Earnings-Call-Transcript.pdf\",\n",
    "        \"followup_qa\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q2/META-Q2-2025-Follow-Up-Call-Transcript.pdf\"\n",
    "    },\n",
    "    \"Q3\": {\n",
    "        \"press_release\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q3/Meta-09-30-2025-Exhibit-99-1-Final.pdf\",\n",
    "        \"presentation\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q3/Earnings-Presentation-Q3-2025-Final.pdf\",\n",
    "        \"earnings_call\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q3/META-Q3-2025-Earnings-Call-Transcript.pdf\",\n",
    "        \"followup_qa\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q3/META-Q3-2025-Follow-Up-Call-Transcript.pdf\"\n",
    "    },\n",
    "    \"Q4\": {\n",
    "        \"press_release\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q4/Meta-12-31-2025-Exhibit-99-1-FINAL.pdf\",\n",
    "        \"presentation\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q4/Earnings-Presentation-Q4-2025-FINAL.pdf\",\n",
    "        \"earnings_call\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q4/META-Q4-2025-Earnings-Call-Transcript.pdf\",\n",
    "        \"followup_qa\": \"https://s21.q4cdn.com/399680738/files/doc_financials/2025/q4/META-Q4-2025-Follow-Up-Call-Transcript.pdf\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Data sources loaded!\")\n",
    "print()\n",
    "print(\"Available quarters: Q1, Q2, Q3, Q4\")\n",
    "print(\"Document types per quarter: press_release, presentation, earnings_call, followup_qa\")\n",
    "print()\n",
    "print(f\"Total documents to process: {len(data_sources) * 4} PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf4f71",
   "metadata": {},
   "source": [
    "## Initialize Wrappers\n",
    "\n",
    "Initialize the LLM Wrappers / Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b18465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import chromadb\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Initialize ChromaDB client (in-memory database, no API keys needed)\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4976dd",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for PDF fetching and chunk storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36cae9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Utility functions defined!\n",
      "\n",
      "Functions available:\n",
      "  - fetch_and_extract_pdf(url): Fetch and extract text from PDFs\n",
      "  - add_chunk_to_collection(): Add chunks to a collection\n",
      "  - query_collection(): Query a collection\n",
      "  - generate_rag_answer(): Generate LLM answers from chunks using GPT-4o\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def fetch_and_extract_pdf(pdf_url, timeout=10, return_type=\"Text\"):\n",
    "    \"\"\"\n",
    "    Fetch a PDF from a URL and extract all text\n",
    "    \n",
    "    Args:\n",
    "        pdf_url: URL to the PDF file\n",
    "        timeout: Request timeout in seconds\n",
    "        return_pages: If True, return list of pages; if False, return combined text\n",
    "    \n",
    "    Returns:\n",
    "        If return_pages=False: Extracted text (string) or None if error\n",
    "        If return_pages=True: List of page texts or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        pdf_file = BytesIO(response.content)\n",
    "        reader = PdfReader(pdf_file)\n",
    "        \n",
    "        if return_type == \"Pages\":\n",
    "            pages = []\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text and page_text.strip():\n",
    "                    pages.append(page_text.strip())\n",
    "            return pages if pages else None\n",
    "        elif return_type == \"Paragraphs\":\n",
    "            paragraphs = []\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    for para in page_text.split(\"\\n\\n\"):\n",
    "                        if para.strip():\n",
    "                            paragraphs.append(para.strip())\n",
    "            return paragraphs if paragraphs else None\n",
    "        else: # Text\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "            \n",
    "            return text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error fetching PDF: {str(e)[:60]}\")\n",
    "        return None\n",
    "\n",
    "def fetch_and_extract_section_from_pdf(pdf_url, timeout=10):\n",
    "    \"\"\"\n",
    "    Fetch a PDF from a URL and extract a specific section using PyMuPDF\n",
    "    \n",
    "    Args:\n",
    "        pdf_url: URL to the PDF file\n",
    "        timeout: Request timeout in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Extracted section text (string) or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(stream=requests.get(pdf_url, timeout=timeout).content, filetype=\"pdf\")\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        \n",
    "        for page_num, page in enumerate(doc):\n",
    "             blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "             \n",
    "             for block in blocks:\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                \n",
    "                for line in block[\"lines\"]:\n",
    "                    line_text = \"\"\n",
    "                    max_font_size = 0\n",
    "                    is_bold = False\n",
    "                    \n",
    "                    for span in line[\"spans\"]:\n",
    "                        for span in line[\"spans\"]:\n",
    "                            text = span[\"text\"].strip()\n",
    "                            if not text:\n",
    "                                continue\n",
    "                            \n",
    "                            line_text += text + \" \"\n",
    "                            font_size = span[\"size\"]\n",
    "                            max_font_size = max(max_font_size, font_size)\n",
    "                            \n",
    "                            if \"bold\" in span[\"font\"].lower():\n",
    "                                is_bold = True\n",
    "                        line_text = line_text.strip()\n",
    "                        \n",
    "                    # Detect section headings (adjust threshold based on your PDF)\n",
    "                    if max_font_size > 11 or is_bold or line_text.isupper():\n",
    "                        # Check if it's actually a heading (not too long)\n",
    "                        if len(line_text) < 100 and line_text:\n",
    "                            # Save previous section\n",
    "                            if current_section and current_section[\"content\"].strip():\n",
    "                                sections.append(current_section)\n",
    "                            \n",
    "                            # Start new section\n",
    "                            current_section = {\n",
    "                                \"title\": line_text,\n",
    "                                \"content\": \"\",\n",
    "                                \"page_start\": page_num + 1,\n",
    "                                \"font_size\": max_font_size\n",
    "                            }\n",
    "                    elif current_section:\n",
    "                        current_section[\"content\"] += line_text + \"\\n\"   \n",
    "                        \n",
    "        # Add last section\n",
    "        if current_section and current_section[\"content\"].strip():\n",
    "            sections.append(current_section)\n",
    "            \n",
    "        return sections if sections else None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error fetching PDF section: {str(e)[:60]}\")\n",
    "        return None\n",
    "\n",
    "def add_chunk_to_collection(collection, doc_type, quarter, chunk_text, chunk_id):\n",
    "    \"\"\"\n",
    "    Add a text chunk to a ChromaDB collection\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection object\n",
    "        doc_type: Type of document (e.g., 'earnings_call')\n",
    "        quarter: Quarter (Q1, Q2, Q3)\n",
    "        chunk_text: The text content of the chunk\n",
    "        chunk_id: Unique identifier for the chunk\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"doc_type\": doc_type,\n",
    "        \"quarter\": quarter,\n",
    "        \"chunk_length\": len(chunk_text)\n",
    "    }\n",
    "    \n",
    "    collection.add(\n",
    "        ids=[chunk_id],\n",
    "        documents=[chunk_text],\n",
    "        metadatas=[metadata]\n",
    "    )\n",
    "\n",
    "def query_collection(collection, doc_type=None, quarter=None):\n",
    "    \"\"\"\n",
    "    Query a ChromaDB collection with optional filters\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection object\n",
    "        doc_type: Optional filter by document type\n",
    "        quarter: Optional filter by quarter\n",
    "    \n",
    "    Returns:\n",
    "        Query results dict with 'documents', 'metadatas', 'ids'\n",
    "    \"\"\"\n",
    "    where_filter = None\n",
    "    conditions = []\n",
    "    \n",
    "    if doc_type:\n",
    "        conditions.append({\"doc_type\": {\"$eq\": doc_type}})\n",
    "    if quarter:\n",
    "        conditions.append({\"quarter\": {\"$eq\": quarter}})\n",
    "    \n",
    "    if conditions:\n",
    "        if len(conditions) == 1:\n",
    "            where_filter = conditions[0]\n",
    "        else:\n",
    "            where_filter = {\"$and\": conditions}\n",
    "    \n",
    "    return collection.get(where=where_filter)\n",
    "\n",
    "def generate_rag_answer(query, chunks, max_chunks=5):\n",
    "    \"\"\"\n",
    "    Generate an answer using GPT-4o based on a query and retrieved chunks\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        chunks: List of text chunks to use as context\n",
    "        max_chunks: Maximum number of chunks to use\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer (string) or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Limit chunks to avoid token overflow\n",
    "        chunks_to_use = chunks[:max_chunks]\n",
    "        context = \"\\n\\n\".join([f\"[Chunk {i+1}]\\n{chunk}\" for i, chunk in enumerate(chunks_to_use)])\n",
    "        \n",
    "        system_prompt = \"\"\"You are a helpful financial analyst assistant. \n",
    "Based on the provided document chunks, answer the user's question accurately and concisely.\n",
    "If the information is not in the provided chunks, say 'I don't have enough information to answer this question.'\n",
    "Always cite which chunks you used to answer the question.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"Question: {query}\n",
    "\n",
    "Document Chunks:\n",
    "{context}\n",
    "\n",
    "Please provide a clear, concise answer based on the above information.\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error generating answer: {str(e)[:60]}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Utility functions defined!\")\n",
    "print()\n",
    "print(\"Functions available:\")\n",
    "print(\"  - fetch_and_extract_pdf(url): Fetch and extract text from PDFs\")\n",
    "print(\"  - add_chunk_to_collection(): Add chunks to a collection\")\n",
    "print(\"  - query_collection(): Query a collection\")\n",
    "print(\"  - generate_rag_answer(): Generate LLM answers from chunks using GPT-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89205808",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB Collections\n",
    "\n",
    "> The details below\n",
    "- Strategy 3 - By Paragraphs\n",
    "- Strategy 4 - By Fixed token length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d864b",
   "metadata": {},
   "source": [
    "### Strategy 1 - By Page\n",
    "\n",
    "> Details below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7458b41",
   "metadata": {},
   "source": [
    "#### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80837dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Strategy 1 - ByPage: BUILDING INDEX: PAGE-BASED CHUNKING (Each Page = 1 Chunk)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”„ Processing Q1...\n",
      "  ğŸ“„ press_release... âœ… 8 pages\n",
      "  ğŸ“„ presentation... âœ… 17 pages\n",
      "  ğŸ“„ earnings_call... âœ… 18 pages\n",
      "  ğŸ“„ followup_qa... âœ… 10 pages\n",
      "\n",
      "ğŸ”„ Processing Q2...\n",
      "  ğŸ“„ press_release... âœ… 8 pages\n",
      "  ğŸ“„ presentation... âœ… 17 pages\n",
      "  ğŸ“„ earnings_call... âœ… 18 pages\n",
      "  ğŸ“„ followup_qa... âœ… 9 pages\n",
      "\n",
      "ğŸ”„ Processing Q3...\n",
      "  ğŸ“„ press_release... âœ… 8 pages\n",
      "  ğŸ“„ presentation... âœ… 17 pages\n",
      "  ğŸ“„ earnings_call... âœ… 18 pages\n",
      "  ğŸ“„ followup_qa... âœ… 9 pages\n",
      "\n",
      "ğŸ”„ Processing Q4...\n",
      "  ğŸ“„ press_release... âœ… 8 pages\n",
      "  ğŸ“„ presentation... âœ… 17 pages\n",
      "  ğŸ“„ earnings_call... âœ… 20 pages\n",
      "  ğŸ“„ followup_qa... âœ… 10 pages\n",
      "\n",
      "âœ… Strategy 1: Added 228 page-based chunks to collection!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Strategy 1 - ByPage: BUILDING INDEX: PAGE-BASED CHUNKING (Each Page = 1 Chunk)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "s1_bypage_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"Strategy1_ByPage_Collection\",\n",
    "    metadata={\"description\": \"Chunks created by treating each PDF page as a separate chunk\"}\n",
    ")\n",
    "\n",
    "s1_chunk_count = 0\n",
    "for quarter in data_sources:\n",
    "    print(f\"\\nğŸ”„ Processing {quarter}...\")\n",
    "    for doc_type in data_sources[quarter]:\n",
    "        print(f\"  ğŸ“„ {doc_type}...\", end=\"\")\n",
    "        \n",
    "        # Fetch PDF data\n",
    "        pdf_url = data_sources[quarter][doc_type]\n",
    "        pages = fetch_and_extract_pdf(pdf_url, return_type=\"Pages\")\n",
    "        \n",
    "        if not pages:\n",
    "            print(f\"   âš ï¸  No text extracted from {pdf_url}\")\n",
    "            continue\n",
    "        \n",
    "        # Strategy 1: Each page is one chunk  \n",
    "        for i, page_text in enumerate(pages):\n",
    "            chunk_id = f\"{quarter}_{doc_type}_s1_page_{i+1}\"\n",
    "            add_chunk_to_collection(s1_bypage_collection, doc_type, quarter, page_text, chunk_id)\n",
    "            s1_chunk_count += 1\n",
    "            \n",
    "        print(f\" âœ… {i} pages\")\n",
    "        \n",
    "print(f\"\\nâœ… Strategy 1: Added {s1_chunk_count} page-based chunks to collection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea1bf7",
   "metadata": {},
   "source": [
    "#### Inspect Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c4151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Strategy 1 - ByPage: INSPECT INDEX : PAGE-BASED CHUNKING (Each Page = 1 Chunk)\n",
      "======================================================================\n",
      "\n",
      "âœ… Total documents in ByPage Index: 228\n",
      "Strategy 1 - ByPage - Statistics:\n",
      "   Average chunk size: 2175 characters\n",
      "   Minimum chunk size: 8 characters\n",
      "   Maximum chunk size: 5375 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Strategy 1 - ByPage: INSPECT INDEX : PAGE-BASED CHUNKING (Each Page = 1 Chunk)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get all documents in the collection\n",
    "s1_all_docs = query_collection(s1_bypage_collection)\n",
    "\n",
    "print(f\"\\nâœ… Total documents in ByPage Index: {len(s1_all_docs['documents'])}\")\n",
    "print(f\"Strategy 1 - ByPage - Statistics:\")\n",
    "s1_chunk_sizes = [m['chunk_length'] for m in s1_all_docs['metadatas']]\n",
    "print(f\"   Average chunk size: {sum(s1_chunk_sizes) / len(s1_chunk_sizes):.0f} characters\")\n",
    "print(f\"   Minimum chunk size: {min(s1_chunk_sizes)} characters\")\n",
    "print(f\"   Maximum chunk size: {max(s1_chunk_sizes)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c187a",
   "metadata": {},
   "source": [
    "### Strategy 2 - By Sentences (3)\n",
    "\n",
    "> Details below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3932e",
   "metadata": {},
   "source": [
    "#### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49951e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Strategy 2 - BySentences: BUILDING INDEX: SENTENCE BASED CHUNKING (3 x Sentences = 1 Chunk)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”„ Processing Q1...\n",
      "  ğŸ“„ press_release... âœ… 108 sentences grouped into 36 chunks\n",
      "  ğŸ“„ presentation... âœ… 99 sentences grouped into 33 chunks\n",
      "  ğŸ“„ earnings_call... âœ… 460 sentences grouped into 154 chunks\n",
      "  ğŸ“„ followup_qa... âœ… 275 sentences grouped into 92 chunks\n",
      "\n",
      "ğŸ”„ Processing Q2...\n",
      "  ğŸ“„ press_release... âœ… 120 sentences grouped into 40 chunks\n",
      "  ğŸ“„ presentation... âœ… 99 sentences grouped into 33 chunks\n",
      "  ğŸ“„ earnings_call... âœ… 459 sentences grouped into 153 chunks\n",
      "  ğŸ“„ followup_qa... âœ… 234 sentences grouped into 78 chunks\n",
      "\n",
      "ğŸ”„ Processing Q3...\n",
      "  ğŸ“„ press_release... âœ… 144 sentences grouped into 48 chunks\n",
      "  ğŸ“„ presentation... âœ… 114 sentences grouped into 38 chunks\n",
      "  ğŸ“„ earnings_call... âœ… 443 sentences grouped into 148 chunks\n",
      "  ğŸ“„ followup_qa... âœ… 243 sentences grouped into 81 chunks\n",
      "\n",
      "ğŸ”„ Processing Q4...\n",
      "  ğŸ“„ press_release... âœ… 128 sentences grouped into 43 chunks\n",
      "  ğŸ“„ presentation... âœ… 115 sentences grouped into 39 chunks\n",
      "  ğŸ“„ earnings_call... âœ… 473 sentences grouped into 158 chunks\n",
      "  ğŸ“„ followup_qa... âœ… 261 sentences grouped into 87 chunks\n",
      "\n",
      "âœ… Strategy 2: Added 1261 sentence-based chunks to collection!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Strategy 2 - BySentences: BUILDING INDEX: SENTENCE BASED CHUNKING (3 x Sentences = 1 Chunk)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "s2_bysentences_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"Strategy2_BySentences_Collection\",\n",
    "    metadata={\"description\": \"Chunks created by grouping every 3 sentences together\"}\n",
    ")\n",
    "\n",
    "s2_chunk_count = 0\n",
    "for quarter in data_sources:\n",
    "    print(f\"\\nğŸ”„ Processing {quarter}...\")\n",
    "    for doc_type in data_sources[quarter]:\n",
    "        print(f\"  ğŸ“„ {doc_type}...\", end=\"\")\n",
    "        \n",
    "        # Fetch PDF data\n",
    "        pdf_url = data_sources[quarter][doc_type]\n",
    "        text = fetch_and_extract_pdf(pdf_url, return_type=\"Text\")\n",
    "        \n",
    "        if not text:\n",
    "            print(f\"   âš ï¸  No text extracted from {pdf_url}\")\n",
    "            continue\n",
    "        \n",
    "        # Strategy 2: Group every 3 sentences into one chunk\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        for i in range(0, len(sentences), 3):\n",
    "            chunk_sentences = sentences[i:i+3]\n",
    "            chunk_text = '. '.join(chunk_sentences) + ('.' if chunk_sentences else '')\n",
    "            chunk_id = f\"{quarter}_{doc_type}_s2_sentences_{i//3 + 1}\"\n",
    "            add_chunk_to_collection(s2_bysentences_collection, doc_type, quarter, chunk_text, chunk_id)\n",
    "            s2_chunk_count += 1\n",
    "            \n",
    "        print(f\" âœ… {len(sentences)} sentences grouped into {len(sentences) // 3 + (1 if len(sentences) % 3 > 0 else 0)} chunks\")\n",
    "        \n",
    "print(f\"\\nâœ… Strategy 2: Added {s2_chunk_count} sentence-based chunks to collection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107bbb9",
   "metadata": {},
   "source": [
    "#### Inspect Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98aaa881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Strategy 2 - BySentence: INSPECT INDEX : SENTENCE BASED CHUNKING (3 x Sentences = 1 Chunk)\n",
      "======================================================================\n",
      "\n",
      "âœ… Total documents in BySentence Index: 1261\n",
      "Strategy 2 - BySentence - Statistics:\n",
      "   Average chunk size: 389 characters\n",
      "   Minimum chunk size: 4 characters\n",
      "   Maximum chunk size: 4926 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Strategy 2 - BySentence: INSPECT INDEX : SENTENCE BASED CHUNKING (3 x Sentences = 1 Chunk)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get all documents in the collection\n",
    "s2_all_docs = query_collection(s2_bysentences_collection)\n",
    "\n",
    "print(f\"\\nâœ… Total documents in BySentence Index: {len(s2_all_docs['documents'])}\")\n",
    "print(f\"Strategy 2 - BySentence - Statistics:\")\n",
    "s2_chunk_sizes = [m['chunk_length'] for m in s2_all_docs['metadatas']]\n",
    "print(f\"   Average chunk size: {sum(s2_chunk_sizes) / len(s2_chunk_sizes):.0f} characters\")\n",
    "print(f\"   Minimum chunk size: {min(s2_chunk_sizes)} characters\")\n",
    "print(f\"   Maximum chunk size: {max(s2_chunk_sizes)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924579d6",
   "metadata": {},
   "source": [
    "### Strategy 3 - By Sections\n",
    "\n",
    "> Details below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bddf44c",
   "metadata": {},
   "source": [
    "#### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2192b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Strategy 3 - BySections: BUILDING INDEX: SECTION BASED CHUNKING (1 Section = 1 Chunk)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”„ Processing Q1...\n",
      "  ğŸ“„ press_release... âœ… Extracted 44 sections\n",
      "  ğŸ“„ presentation...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q1/Earnings-Presentation-Q1-2025-FINAL.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "  ğŸ“„ earnings_call...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q1/Transcripts/META-Q1-2025-Earnings-Call-Transcript-1.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "  ğŸ“„ followup_qa... âœ… Extracted 11 sections\n",
      "\n",
      "ğŸ”„ Processing Q2...\n",
      "  ğŸ“„ press_release... âœ… Extracted 43 sections\n",
      "  ğŸ“„ presentation...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_downloads/Earnings-Presentation-Q2-2025-FINAL.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "  ğŸ“„ earnings_call...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q2/META-Q2-2025-Earnings-Call-Transcript.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "  ğŸ“„ followup_qa...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q2/META-Q2-2025-Follow-Up-Call-Transcript.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "\n",
      "ğŸ”„ Processing Q3...\n",
      "  ğŸ“„ press_release... âœ… Extracted 48 sections\n",
      "  ğŸ“„ presentation...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q3/Earnings-Presentation-Q3-2025-Final.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "  ğŸ“„ earnings_call...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q3/META-Q3-2025-Earnings-Call-Transcript.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "  ğŸ“„ followup_qa...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q3/META-Q3-2025-Follow-Up-Call-Transcript.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "\n",
      "ğŸ”„ Processing Q4...\n",
      "  ğŸ“„ press_release... âœ… Extracted 49 sections\n",
      "  ğŸ“„ presentation...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q4/Earnings-Presentation-Q4-2025-FINAL.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "  ğŸ“„ earnings_call... âœ… Extracted 1 sections\n",
      "  ğŸ“„ followup_qa...   âš ï¸  No sections extracted from https://s21.q4cdn.com/399680738/files/doc_financials/2025/q4/META-Q4-2025-Follow-Up-Call-Transcript.pdf\n",
      "   Falling back to Sentence-based chunking...\n",
      "\n",
      "âœ… Strategy 3: Added 356 section-based chunks to collection!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Strategy 3 - BySections: BUILDING INDEX: SECTION BASED CHUNKING (1 Section = 1 Chunk)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "s3_bysections_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"Strategy3_BySections_Collection\",\n",
    "    metadata={\"description\": \"Chunks created by treating each section as a separate chunk\"}\n",
    ")\n",
    "\n",
    "s3_chunk_count = 0\n",
    "for quarter in data_sources:\n",
    "    print(f\"\\nğŸ”„ Processing {quarter}...\")\n",
    "    for doc_type in data_sources[quarter]:\n",
    "        print(f\"  ğŸ“„ {doc_type}...\", end=\"\")\n",
    "        \n",
    "        # Fetch PDF data\n",
    "        pdf_url = data_sources[quarter][doc_type]\n",
    "        # paragraphs = fetch_and_extract_pdf(pdf_url, return_type=\"Paragraphs\")\n",
    "        \n",
    "        # if not paragraphs:\n",
    "        #     print(f\"   âš ï¸  No text extracted from {pdf_url}\")\n",
    "        #     continue\n",
    "        \n",
    "        # print(f\"   Extracted {len(paragraphs)} paragraphs from PDF\", end=\"\")\n",
    "        # # Strategy 3: Each paragraph is one chunk  \n",
    "        # for i, para_text in enumerate(paragraphs):\n",
    "        #     print(\".\", end=\"\")  # Progress indicator for large number of paragraphs\n",
    "        #     chunk_id = f\"{quarter}_{doc_type}_s3_para_{i+1}\"\n",
    "        #     add_chunk_to_collection(s3_byparagraphs_collection, doc_type, quarter, para_text, chunk_id)\n",
    "        #     s3_chunk_count += 1\n",
    "            \n",
    "        # print(f\" âœ… {len(paragraphs)} paragraphs\")\n",
    "        sections = fetch_and_extract_section_from_pdf(pdf_url)\n",
    "        if not sections or len(sections) == 0:\n",
    "            print(f\"   âš ï¸  No sections extracted from {pdf_url}\")\n",
    "            print(f\"   Falling back to Sentence-based chunking...\")\n",
    "            pages = fetch_and_extract_pdf(pdf_url, return_type=\"Pages\")\n",
    "            if not pages:\n",
    "                print(f\"   âš ï¸  No text extracted from {pdf_url}\")\n",
    "                continue\n",
    "            for i, page_text in enumerate(pages):\n",
    "                chunk_id = f\"{quarter}_{doc_type}_s1_page_{i+1}\"\n",
    "                add_chunk_to_collection(s3_bysections_collection, doc_type, quarter, page_text, chunk_id)\n",
    "                s3_chunk_count += 1\n",
    "            # Add pages to collection as fallback\n",
    "            continue\n",
    "        \n",
    "        print(f\" âœ… Extracted {len(sections) if sections else 0} sections\")\n",
    "        for section in sections:\n",
    "                chunk_id = f\"{quarter}_{doc_type}_s3_section_{s3_chunk_count + 1}\"\n",
    "                add_chunk_to_collection(s3_bysections_collection, doc_type, quarter, section['content'], chunk_id)\n",
    "                s3_chunk_count += 1\n",
    "            \n",
    "print(f\"\\nâœ… Strategy 3: Added {s3_chunk_count} section-based chunks to collection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77430f",
   "metadata": {},
   "source": [
    "#### Inspect Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e84a9b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Strategy 3 - ByParagraphs: INSPECT INDEX : PARAGRAPH BASED CHUNKING (1 Paragraph = 1 Chunk)\n",
      "======================================================================\n",
      "\n",
      "âœ… Total documents in BySection Index: 356\n",
      "Strategy 3 - BySections - Statistics:\n",
      "   Average chunk size: 1299 characters\n",
      "   Minimum chunk size: 6 characters\n",
      "   Maximum chunk size: 38090 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Strategy 3 - ByParagraphs: INSPECT INDEX : PARAGRAPH BASED CHUNKING (1 Paragraph = 1 Chunk)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get all documents in the collection\n",
    "s3_all_docs = query_collection(s3_bysections_collection)\n",
    "print(f\"\\nâœ… Total documents in BySection Index: {len(s3_all_docs['documents'])}\")\n",
    "print(f\"Strategy 3 - BySections - Statistics:\")\n",
    "s3_chunk_sizes = [m['chunk_length'] for m in s3_all_docs['metadatas']]\n",
    "print(f\"   Average chunk size: {sum(s3_chunk_sizes) / len(s3_chunk_sizes):.0f} characters\")\n",
    "print(f\"   Minimum chunk size: {min(s3_chunk_sizes)} characters\")\n",
    "print(f\"   Maximum chunk size: {max(s3_chunk_sizes)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2024b3",
   "metadata": {},
   "source": [
    "### Strategy 4 - Fixed Token Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d93c56",
   "metadata": {},
   "source": [
    "#### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d72f4f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Strategy 4 - Fixed Token: BUILDING INDEX: FIXED TOKEN CHUNKING (1 Chunk = Fixed Token Length)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”„ Processing Q1...\n",
      "  ğŸ“„ press_release... âœ… 2519 words split into 3 chunks\n",
      "  ğŸ“„ presentation... âœ… 2676 words split into 3 chunks\n",
      "  ğŸ“„ earnings_call... âœ… 9344 words split into 10 chunks\n",
      "  ğŸ“„ followup_qa... âœ… 5372 words split into 6 chunks\n",
      "\n",
      "ğŸ”„ Processing Q2...\n",
      "  ğŸ“„ press_release... âœ… 2974 words split into 3 chunks\n",
      "  ğŸ“„ presentation... âœ… 2668 words split into 3 chunks\n",
      "  ğŸ“„ earnings_call... âœ… 9622 words split into 10 chunks\n",
      "  ğŸ“„ followup_qa... âœ… 4607 words split into 5 chunks\n",
      "\n",
      "ğŸ”„ Processing Q3...\n",
      "  ğŸ“„ press_release... âœ… 3277 words split into 4 chunks\n",
      "  ğŸ“„ presentation... âœ… 2861 words split into 3 chunks\n",
      "  ğŸ“„ earnings_call... âœ… 9682 words split into 10 chunks\n",
      "  ğŸ“„ followup_qa... âœ… 4645 words split into 5 chunks\n",
      "\n",
      "ğŸ”„ Processing Q4...\n",
      "  ğŸ“„ press_release... âœ… 3019 words split into 4 chunks\n",
      "  ğŸ“„ presentation... âœ… 2925 words split into 3 chunks\n",
      "  ğŸ“„ earnings_call... âœ… 9995 words split into 10 chunks\n",
      "  ğŸ“„ followup_qa... âœ… 5080 words split into 6 chunks\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Strategy 4 - Fixed Token: BUILDING INDEX: FIXED TOKEN CHUNKING (1 Chunk = Fixed Token Length)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "s4_fixedtoken_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"Strategy4_FixedToken_Collection\",\n",
    "    metadata={\"description\": \"Chunks created by splitting text into fixed token lengths (e.g., 1000 tokens per chunk)\"}\n",
    ")\n",
    "\n",
    "s4_chunk_count = 0\n",
    "for quarter in data_sources:\n",
    "    print(f\"\\nğŸ”„ Processing {quarter}...\")\n",
    "    for doc_type in data_sources[quarter]:\n",
    "        print(f\"  ğŸ“„ {doc_type}...\", end=\"\")\n",
    "        \n",
    "        # Fetch PDF data\n",
    "        pdf_url = data_sources[quarter][doc_type]\n",
    "        text = fetch_and_extract_pdf(pdf_url, return_type=\"Text\")\n",
    "        \n",
    "        if not text:\n",
    "            print(f\"   âš ï¸  No text extracted from {pdf_url}\")\n",
    "            continue\n",
    "        \n",
    "        # Strategy 4: Split text into fixed token length chunks (e.g., 1000 tokens)\n",
    "        # For simplicity, we'll approximate tokens by splitting on whitespace and punctuation\n",
    "        chunk_size = 1000  # Approximate number of tokens per chunk\n",
    "        overlap = 200    # Number of words to overlap between chunks to maintain context\n",
    "        \n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "             end = start + chunk_size\n",
    "             chunk_text = text[start:end]\n",
    "             \n",
    "             chunk_id = f\"{quarter}_{doc_type}_s4_fixedtoken_{s4_chunk_count + 1}\"\n",
    "             add_chunk_to_collection(s4_fixedtoken_collection, doc_type, quarter, chunk_text, chunk_id)\n",
    "             s4_chunk_count += 1\n",
    "             start += (chunk_size - overlap)  # Move start forward with overlap\n",
    "            \n",
    "        print(f\" âœ… {len(text.split())} words split into {len(text.split()) // chunk_size + (1 if len(text.split()) % chunk_size > 0 else 0)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b652f",
   "metadata": {},
   "source": [
    "#### Inspect Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6a53108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Strategy 4 - Fixed Token: INSPECT INDEX: FIXED TOKEN CHUNKING (1 Chunk = Fixed Token Length)\n",
      "======================================================================\n",
      "\n",
      "âœ… Total documents in Fixed Token Index: 630\n",
      "Strategy 4 - Fixed Token - Statistics:\n",
      "   Average chunk size: 982 characters\n",
      "   Minimum chunk size: 4 characters\n",
      "   Maximum chunk size: 1000 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š Strategy 4 - Fixed Token: INSPECT INDEX: FIXED TOKEN CHUNKING (1 Chunk = Fixed Token Length)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get all documents in the collection\n",
    "s4_all_docs = query_collection(s4_fixedtoken_collection)\n",
    "print(f\"\\nâœ… Total documents in Fixed Token Index: {len(s4_all_docs['documents'])}\")\n",
    "print(f\"Strategy 4 - Fixed Token - Statistics:\")\n",
    "s4_chunk_sizes = [m['chunk_length'] for m in s4_all_docs['metadatas']]\n",
    "print(f\"   Average chunk size: {sum(s4_chunk_sizes) / len(s4_chunk_sizes):.0f} characters\")\n",
    "print(f\"   Minimum chunk size: {min(s4_chunk_sizes)} characters\")\n",
    "print(f\"   Maximum chunk size: {max(s4_chunk_sizes)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe88af",
   "metadata": {},
   "source": [
    "## Index Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25dea87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š INDEX COMPARISON:\n",
      "----------------------------------------------------------------------\n",
      "Strategy 1 - ByPage     : Documents =   228 | Avg Size =   2175 chars\n",
      "Strategy 2 - BySentences: Documents =  1261 | Avg Size =    389 chars\n",
      "Strategy 3 - BySections : Documents =   356 | Avg Size =   1299 chars\n",
      "Strategy 4 - FixedToken : Documents =   630 | Avg Size =    982 chars\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nğŸ“Š INDEX COMPARISON:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"Strategy 1 - ByPage     : Documents = {len(s1_all_docs['documents']):5d} | Avg Size = {sum([m['chunk_length'] for m in s1_all_docs['metadatas']]) / len(s1_all_docs['metadatas']):6.0f} chars\")\n",
    "print(f\"Strategy 2 - BySentences: Documents = {len(s2_all_docs['documents']):5d} | Avg Size = {sum([m['chunk_length'] for m in s2_all_docs['metadatas']]) / len(s2_all_docs['metadatas']):6.0f} chars\")\n",
    "print(f\"Strategy 3 - BySections : Documents = {len(s3_all_docs['documents']):5d} | Avg Size = {sum([m['chunk_length'] for m in s3_all_docs['metadatas']]) / len(s3_all_docs['metadatas']):6.0f} chars\")\n",
    "print(f\"Strategy 4 - FixedToken : Documents = {len(s4_all_docs['documents']):5d} | Avg Size = {sum([m['chunk_length'] for m in s4_all_docs['metadatas']]) / len(s4_all_docs['metadatas']):6.0f} chars\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
